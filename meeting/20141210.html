<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz:300,400);
      @import url(http://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300);
      @import url(http://fonts.googleapis.com/css?family=Open+Sans:300,400);

      h1 , h3, h5{
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
      }
      h2, h4 { font-family: 'Open Sans Condensed'; 

      }
      body{
        font-family: 'Open Sans';
        font-weight: 300;
      }
      .remark-slide-content h1 { font-size: 2.3em; }
      .remark-slide-content h2 { font-size: 1.6em; }
      h5 { font-size: 1.2em; }
      h4 { font-size: 1.2em;}
      
      li p { line-height : 1.1em; }
      li { line-height: 27px;
           font-size: 20px;
           margin-top: 5px;
      }
      img {
        width: 100%;
        
      }
      .img-30{
        display: block;
        
        /*margin-right: auto;
        margin-left: auto;*/
       width: 30%;
      }
      .img-40{
        display: block;
        
        /*margin-right: auto;
        margin-left: auto;*/
       width: 40%;
      }
      .img-60{
        display: block;
        margin-top: 1em;
        margin-right: auto;
        margin-left: auto;
        width: 60%;
      }
      .img-70{
        display: block;
        margin-top: 1em;
        margin-right: auto;
        margin-left: auto;
        width: 70%;
      }
      .img-80{
        display: block;
        margin-top: 1em;
        margin-right: auto;
        margin-left: auto;
        width: 80%;
      }

      .red { color: #fa0000}
      .orange { color: #F59C25;}
      .green { color: rgb(55, 126, 39);}
      .blue {color: #257FF5;}
      .pink {color: rgb(244, 10, 97);}
      .darkgray {color: #474747;}

      .small-font{
        font-size: 0.6em;
        line-height: 0.4em;
      }

      .right-column{
        width: 50%;
        float: right;
        
      }

      .right-column li {
        font-size: 0.8em;
        line-height: 1.2em;
        margin-bottom: 0.6em;
      }
      .right-column img{
        width:100%;
      }

      .left-column{
        width: 50%;
        float: left; 
        font-size: 0.8em;
      }
      .left-column img{
        width:100%;
      }

      .img-adjust{
        width: 50%;
        margin-left: auto;
        margin-right: auto;
      }
      .tab{
        margin-left: 1em;
        margin-top: 0.5em;
      }
      .footnote{
        font-family: 'Open Sans Condensed';
        position: absolute;
        bottom: 0.5em;
        color: black;
        color: #474747;
        font-size: 0.7em;
      }
      .headnote{
        font-family: 'Open Sans Condensed';
        position: absolute;
        top: 1em;
        color: #474747;
        font-size: 1.0em;
      }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle
Using Speaker's Referential Intentions to Model Early Cross-Situational Word Learning
===============
.darkgray[
##### Michael C. Frank, Noah D. Goodman, and Joshua B. Tenenbaum</br>Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology #####]

.headnote[
Psychological Science]

---

## Abstract

* Word learning is a "chicken and egg" problems.</br>
If a child could understand speakers' utterances, it would be easy to learn the meanings of individual words, and once a child knows what many words mean, it is easy to infer speakers' intended meanings.
* This article describes a .pink[computaional model of word learning] that solves these two inference problems inparallel.
* The model inferred .pink[parings between words and object concepts] with higher precision than comparison models.
* By making probabilistic inferences about spekers' intentions, the model explains a variety of .pink[behavioral phenomena] described in the word-learning literature.

---

## Literature Review

* When children learn their first words, they face a challenging joint-inference problem: They are both trying to infer what meaning a speaker is attempting to communicate at the moment a sentence is uttered and trying to learn the more stable mappings between words and referents that constitute the lexicon of their language.
* For a learner without either of these pieces of information, word learning is a hard computational problem.
---
## Intention of speakers

* Social theories suggest that learners rely on a rich understanding of the .pink[goals and intentions of speakers] and assume that—at least in the case of object nouns—once the child understands what is being talked about, the mappings between words and referents are relatively easy to learn.
* These theories must assume some mechanism for making mappings, but this mechanism is often taken to be deterministic, and its details are rarely specified.

---
## Cross-situational word learning

* Word learning take advantage of the fact that words often refer to the immediate environment of the speaker, which allows learners to build a lexicon based on consistent associations between words and their referents.
* There are some existing models using methods such as connectionist, deductive, competition-based and probabilistic have had sucess in accounting for many phenomena in word learning.
* Competition-based Model: The competition model uses a .pink[feature representation] of the referent, and determines the activation of a feature set (i.e., a referent) for each of the lexical choices based on the sum of the association of the individual features previously seen with each word. This strategy yields a .pink[mutual exclusivity] effect, and accounts for the reluctance to learn second labels. [1]
* However, speakers often talk about objects that are not visible and about actions that are not in progress at the moment of speech. (the .pink[displacement] feature of language.)
* Cross-situational models treat the complex and variable communicative intentions of speakers as noise to be averaged out via repeated observation or minimized via the use of attentional cues, rather than as an important aspect of communication to be used in the learning task.

.footnote[[1] Computational Modeling of Human Language Acquisition, Afra Alishahi]

---
## Design of the Model

.left-column[
![alt text](img/14.png)
*O*: the objects presented in the context<br>
*I*: the objects that the speaker intends to refer to<br>
*W*: the words that the speaker utters<br>
*S*: set of objects presented, the situation <br>
*L*: the lexicon of the speaker's language. (.pink[the target the learner wants to learn])
]
.right-column[
####Assumptions
* What speakers intend to say is a function of the physical world around them.
* The words speakers utter are a function of what the speakers intend to say and how those intentions can be translated into the language they are speaking.
* Limit the models to the task of learning names for objects.
* Objects are identified as instances of basic-level object categories, putting aside the challenge of identifying the particular aspect of an object being named
]

---
## Mathematics
* The model defines a probability distribution over unobserved lexicons L and the observed corpus C of situations. Our goal is to infer the lexicon with the highest posterior prob- ability.We find this posterior probability using Bayes' rule:<br>
.img-30[
![alt text](img/15.png)]
* Making lexicons exponentially less probable as they included more word-object pairings and the equation becomes: 
.img-30[
![alt text](img/16.png)]
* The likelihood term captures .pink[the learner's assumptions] about the structure of the learning task. 

---
## Mathematics (cont.)
* *W* and *O* are conditionaaly independent given *I*.
* Since we can't directly observe the speaker's referential intention, we sum over all possible values of *I.small-font[s]* under the constraint that *I.small-font[s]* consists of *O*. (the relevant subset of possible intentions consists of those that refer to a subset of the objects in the physical context.)
.img-60[
![alt text](img/21.png)
]
* Finally, 
.img-60[
![alt text](img/17.png)]
gamma is the probability that a word is used referentially in any given context.


---
## Corpus Evaluation
* Data Source: coded two video files (me06 and di03, each approximately 10 min long) from the Rollins section of the Child Language Data Exchange System (CHILDES)
![alt text](img/18.png)

* The distinction between referential and nonreferential words al- lowed our model to exclude from the lexicon words that were used without a consistent referent
* The ability of the model to infer an empty intention allowed it to discount utter- ances that did not contain references to any object in the im- mediate context.

---
## Prediction of experimental results
* The model prefers sparse lexicons because the simplicity prior biases the model against adding word-object mappings that do not increase the likelihood of the data.
* The model tends to prefer one-to-one lexicons if they are con- sistent with the observed data, because having multiple words that can refer to an object reduces the probability of any single word being used consistently to refer to that object.
* The model prefers that people have intentions to talk about the ob- jects that are present, because words that are generated refer- entially from an intention to talk about an object have higher likelihood than words that are generated nonreferentially at random from the entire vocabulary of the language.

---
## Mutual Exclusivity
.img-70[
![alt text](img/19.png)
]

* Childeren posses a principle of mutual exclusivity that leads them to prefer lexicons with only one label for each objects.

---
## Object Individuation
.img-70[
![alt text](img/20.png)
]
* surprisal (negative log probability), a measure that has previously been used successfully to link model probabilities to human reaction time data.
* This comparison can be interpreted as measuring, for a learner with no knowledge of what the words mean, how much more or less surprising it would be to find one object as opposed to two behind the screen.
* The model was able to use its assumptions about how words work to make inferences about the states of the world that caused a speaker to produce particular utterances.
---
## Intention Reading
* The model, built around inferring the speaker’s intended referents, can capture this interpretation directly.
* To illustrate this point, we constructed a situation with two novel objects and a single novel word.
* The model then highly preferred the correct pairing.

---
class: center, middle

Questions or Comments :)
=============


    </textarea>
    <script src="http://gnab.github.io/remark/downloads/remark-latest.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create();
    </script>
  </body>
</html>