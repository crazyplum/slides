{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Processing\n",
    "* This tutorial just provide a really basic guide of NLP with python.\n",
    "* If you want to learn more about NLP, you could take courses or check it on Coursera. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLTK Library\n",
    "* The most famous python natural language processing toolkit.\n",
    "* With lots of corpora and and lexical resources!!\n",
    "* O’Reilly publish the Natural Language Processing with Python book, NLTK is the toolkit in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Install NLTK\n",
    "`sudo pip3 install nltk` <br>\n",
    "`sudo pip3 install numpy`  to speed up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization, Stemming, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## To use nltk.tokenize.word_tokenize() function you first need to download data with nltk.download()\n",
    "from nltk import download\n",
    "download()  ## would open a GUI interface\n",
    "## download punct package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey!', \"I don't like this. \"]\n",
      "['Hey', '!']\n",
      "['I', 'do', \"n't\", 'like', 'this', '.']\n",
      "['Hey', '!', 'I', 'do', \"n't\", 'like', 'this', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "s = \"Hey! I don't like this. \"\n",
    "sents = sent_tokenize(s)\n",
    "print(sents)\n",
    "[print(word_tokenize(x)) for x in sents]\n",
    "words = word_tokenize(s)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stemming\n",
    "* Reduce terms to their stems in information retrieval\n",
    "* Most famous one: Porter's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cri\n",
      "string\n",
      "relat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cats'))\n",
    "print(stemmer.stem('crying'))\n",
    "print(stemmer.stem('string'))\n",
    "print(stemmer.stem('relational'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lemmatization\n",
    "* Reduce inflections or variant forms to base form\n",
    "* NLTK's lemmatizer is based on [WordNet](http://wordnetweb.princeton.edu/perl/webwn), a large lexical database of English. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "be\n",
      "drive\n",
      "medium\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer   ## remember to download wornet corpora first\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cats'))\n",
    "print(lemmatizer.lemmatize('is', pos='v'))\n",
    "print(lemmatizer.lemmatize('drove', pos='v'))\n",
    "print(lemmatizer.lemmatize('media'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Simiarity\n",
    "* Use WordNet synset to calculate.\n",
    "* [word].[pos].[number].[lemma]” string where: <br>\n",
    "[word] is the morphological stem identifying the synset. <br>\n",
    "[pos] is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB <br>\n",
    "[number] is the sense number, counting from 0. <br>\n",
    "[lemma] is the morphological form of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "print(cat.path_similarity(cat))  ## count the shortest path of the two sense, from 0-1\n",
    "print(dog.path_similarity(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "move = wn.synset('move.v.01')\n",
    "stop = wn.synset('stop.v.01')\n",
    "print(wn.path_similarity(move, stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practice\n",
    "* Write a simple typing suggestion function.\n",
    "* Use `nltk.coupus.brown` as training data.\n",
    "* You can use `bigrams()` and `ConditionalFreqDist()` to handle data.\n",
    "* Use `input()` to get user input.\n",
    "* The user can choose their next words to type based on your suggestion. Give them 5 suggestions for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.util import bigrams\n",
    "words = brown.words()\n",
    "cfdist = ConditionalFreqDist(bigrams(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(cfdist['I'].most_common(3))\n",
    "print(cfdist['United'].most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "* [NLTK Online Book](http://www.nltk.org/book/ch01.html)\n",
    "* [NLTK Documentation](http://www.nltk.org/index.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
